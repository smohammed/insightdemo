{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of beers = 4813\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "import codecs\n",
    "import gensim\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "allbeers = pd.read_csv('../beers/beerlist.csv')\n",
    "print('number of beers = '+(str(len(allbeers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    return text\n",
    "\n",
    "allbeers['words'] = allbeers['words'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b5b9e09ec7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallbeers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallbeers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/lib/python3.7/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "random.sample(allbeers['words'][0].split(), len(allbeers['words'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(allbeers)):\n",
    "    allbeers['words'][i] = allbeers['words'][i].split()\n",
    "#    try:\n",
    "#        allbeers['words'][i] = random.sample(allbeers['words'][i], 2000)\n",
    "#    except ValueError:\n",
    "allbeers['words'][i] = random.sample(allbeers['words'][i], len(allbeers['words'][i]))\n",
    "allbeers['words'][i] = ' '.join(allbeers['words'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Function to remove stop words from sentences & lemmatize words.\n",
    "def clean(beer):\n",
    "    stop_free = \" \".join([i for i in beer.lower().split() if i not in stop])\n",
    "    normalized = \" \".join(lemma.lemmatize(word,'v') for word in stop_free.split())\n",
    "    x = normalized.split()\n",
    "    y = [s for s in x if len(s) > 2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168091 different terms in the corpus\n"
     ]
    }
   ],
   "source": [
    "#creating the dictionary\n",
    "dictionary = gensim.corpora.Dictionary([\" \".join(allbeers['words']).split()]) \n",
    "print('{} different terms in the corpus'.format(len(dictionary)))\n",
    "#creating the bag of words object\n",
    "bow_corpus = [dictionary.doc2bow(text.split()) for text in allbeers['words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(bow_corpus) # creating the tf-idf model\n",
    "tfidf_corpus = tfidf_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 14\n",
    "lda_model_tfidf = gensim.models.LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=total_topics, passes=5)\n",
    "lda_model_bow = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=total_topics, passes=5)\n",
    "ldatfidffile = open('lda_model_beers_14topics_tfidf.pkl','wb')\n",
    "pickle.dump(lda_model_tfidf, ldatfidffile)\n",
    "ldatfidffile.close()\n",
    "ldabowfile = open('lda_model_beers_14topics_bow.pkl','wb')\n",
    "pickle.dump(lda_model_bow, ldabowfile)\n",
    "ldabowfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-349fe193786e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "data = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_words = allbeers['words']\n",
    "#beers = open(\"docs_beers.pkl\",'wb')\n",
    "#pickle.dump(beer_words, beers)\n",
    " \n",
    "# Use 3500 beers for training.\n",
    "#beer_train, beer_test = train_test_split(beer_words, test_size=0.20)\n",
    "beer_train = allbeers['words'][:3500]\n",
    "beer_test = allbeers['words'][:3500].reset_index(drop=True)\n",
    "\n",
    "# Cleaning all the train beers\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "#beer_clean = [clean(beer) for beer in beer_train]\n",
    "beer_clean = [clean(beer) for beer in beer_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# Creating term dictionary of corpus, where each unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(beer_clean)\n",
    "\n",
    "# Filter terms which occurs in less than 10 beers & more than 90% of the comments\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.9)\n",
    " \n",
    "# List of few words which are removed from dictionary as they are content neutral\n",
    "stoplist = set('bottle bad alcohol strong abv better good nothing beer beers also use make people know many call include part find become like mean often different usually take wikt come give well get since type list say change see refer actually iii aisne kinds pas ask would way something need things want every str'.split())\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "beer_term_matrix = [dictionary.doc2bow(beer) for beer in beer_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.010*\"corn\" ', ' 0.008*\"bottle\" ', ' 0.007*\"clear\" ', ' 0.006*\"water\" ', ' 0.006*\"grain\" ', ' 0.006*\"even\" ', ' 0.006*\"watery\" ', ' 0.005*\"lager\" ', ' 0.005*\"buy\" ', ' 0.005*\"time\"'] \n",
      "\n",
      "['0.031*\"sour\" ', ' 0.023*\"tart\" ', ' 0.014*\"fruit\" ', ' 0.013*\"bottle\" ', ' 0.013*\"funk\" ', ' 0.011*\"cherry\" ', ' 0.010*\"oak\" ', ' 0.009*\"tartness\" ', ' 0.008*\"red\" ', ' 0.007*\"wine\"'] \n",
      "\n",
      "['0.034*\"yeast\" ', ' 0.028*\"belgian\" ', ' 0.019*\"spice\" ', ' 0.014*\"bottle\" ', ' 0.013*\"banana\" ', ' 0.011*\"cleave\" ', ' 0.011*\"fruit\" ', ' 0.010*\"spicy\" ', ' 0.009*\"fruity\" ', ' 0.008*\"golden\"'] \n",
      "\n",
      "['0.029*\"caramel\" ', ' 0.021*\"amber\" ', ' 0.013*\"ale\" ', ' 0.012*\"malty\" ', ' 0.010*\"bottle\" ', ' 0.010*\"clear\" ', ' 0.009*\"copper\" ', ' 0.009*\"toast\" ', ' 0.008*\"rye\" ', ' 0.008*\"bitter\"'] \n",
      "\n",
      "['0.054*\"bourbon\" ', ' 0.050*\"vanilla\" ', ' 0.036*\"barrel\" ', ' 0.036*\"oak\" ', ' 0.016*\"age\" ', ' 0.014*\"maple\" ', ' 0.013*\"coconut\" ', ' 0.013*\"bottle\" ', ' 0.012*\"brown\" ', ' 0.011*\"whiskey\"'] \n",
      "\n",
      "['0.014*\"lager\" ', ' 0.014*\"crisp\" ', ' 0.013*\"golden\" ', ' 0.013*\"clear\" ', ' 0.012*\"pale\" ', ' 0.011*\"clean\" ', ' 0.011*\"bottle\" ', ' 0.010*\"grain\" ', ' 0.009*\"yellow\" ', ' 0.008*\"grassy\"'] \n",
      "\n",
      "['0.030*\"smoke\" ', ' 0.020*\"dark\" ', ' 0.015*\"roast\" ', ' 0.011*\"brown\" ', ' 0.009*\"bitter\" ', ' 0.008*\"bottle\" ', ' 0.008*\"creamy\" ', ' 0.008*\"flavour\" ', ' 0.008*\"black\" ', ' 0.007*\"chocolate\"'] \n",
      "\n",
      "['0.031*\"citrus\" ', ' 0.022*\"ipa\" ', ' 0.020*\"orange\" ', ' 0.020*\"pine\" ', ' 0.017*\"grapefruit\" ', ' 0.014*\"bitter\" ', ' 0.012*\"fruit\" ', ' 0.011*\"hoppy\" ', ' 0.010*\"tropical\" ', ' 0.008*\"hazy\"'] \n",
      "\n",
      "['0.048*\"brown\" ', ' 0.038*\"dark\" ', ' 0.023*\"roast\" ', ' 0.019*\"chocolate\" ', ' 0.019*\"caramel\" ', ' 0.011*\"tan\" ', ' 0.010*\"toast\" ', ' 0.009*\"bottle\" ', ' 0.009*\"ale\" ', ' 0.008*\"malty\"'] \n",
      "\n",
      "['0.024*\"caramel\" ', ' 0.016*\"bottle\" ', ' 0.014*\"fruit\" ', ' 0.013*\"dark\" ', ' 0.011*\"toffee\" ', ' 0.011*\"brown\" ', ' 0.010*\"sugar\" ', ' 0.010*\"warm\" ', ' 0.009*\"honey\" ', ' 0.009*\"big\"'] \n",
      "\n",
      "['0.056*\"spice\" ', ' 0.044*\"pumpkin\" ', ' 0.030*\"cinnamon\" ', ' 0.019*\"ginger\" ', ' 0.019*\"pepper\" ', ' 0.015*\"nutmeg\" ', ' 0.012*\"spicy\" ', ' 0.012*\"winter\" ', ' 0.011*\"ale\" ', ' 0.009*\"heat\"'] \n",
      "\n",
      "['0.059*\"dark\" ', ' 0.035*\"fruit\" ', ' 0.034*\"brown\" ', ' 0.017*\"sugar\" ', ' 0.014*\"caramel\" ', ' 0.014*\"bottle\" ', ' 0.011*\"raisins\" ', ' 0.010*\"raisin\" ', ' 0.008*\"tan\" ', ' 0.008*\"molasses\"'] \n",
      "\n",
      "['0.051*\"wheat\" ', ' 0.020*\"lemon\" ', ' 0.016*\"citrus\" ', ' 0.015*\"orange\" ', ' 0.014*\"banana\" ', ' 0.012*\"hazy\" ', ' 0.011*\"refresh\" ', ' 0.011*\"yellow\" ', ' 0.009*\"coriander\" ', ' 0.008*\"summer\"'] \n",
      "\n",
      "['0.050*\"coffee\" ', ' 0.049*\"chocolate\" ', ' 0.033*\"dark\" ', ' 0.030*\"roast\" ', ' 0.029*\"black\" ', ' 0.023*\"stout\" ', ' 0.011*\"creamy\" ', ' 0.010*\"porter\" ', ' 0.009*\"tan\" ', ' 0.009*\"brown\"'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from gensim.models import LdaMulticore as Lda\n",
    "from gensim.models import LdaModel as Lda\n",
    "\n",
    "# Creating the object for LDA model using gensim library & Training LDA model on the document term matrix.\n",
    "#ldamodel = Lda(beer_term_matrix, workers=3, num_topics=14, id2word = dictionary, passes=10, iterations=200)\n",
    "ldamodel = Lda(beer_term_matrix, num_topics=14, id2word = dictionary, passes=10, iterations=200)\n",
    "\n",
    "# dump LDA model using cPickle for future use\n",
    "ldafile = open('lda_model_beers_14topics_allbeers.pkl','wb')\n",
    "pickle.dump(ldamodel,ldafile)\n",
    "ldafile.close()\n",
    " \n",
    "# Print all the 50 topics\n",
    "for i,topic in enumerate(ldamodel.print_topics(num_topics=14, num_words=10)):\n",
    "   words = topic[1].split(\"+\")\n",
    "   print(words,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.037*\"chocolate\" ', ' 0.035*\"dark\" ', ' 0.033*\"coffee\" ', ' 0.029*\"roast\" ', ' 0.021*\"black\" ', ' 0.016*\"brown\" ', ' 0.015*\"stout\" ', ' 0.010*\"porter\" ', ' 0.010*\"tan\" ', ' 0.009*\"creamy\"'] \n",
      "\n",
      "['0.048*\"spice\" ', ' 0.045*\"pumpkin\" ', ' 0.030*\"cinnamon\" ', ' 0.021*\"ginger\" ', ' 0.014*\"nutmeg\" ', ' 0.010*\"ale\" ', ' 0.009*\"pie\" ', ' 0.007*\"vanilla\" ', ' 0.007*\"brown\" ', ' 0.007*\"dark\"'] \n",
      "\n",
      "['0.027*\"dark\" ', ' 0.024*\"smoke\" ', ' 0.016*\"rye\" ', ' 0.016*\"brown\" ', ' 0.014*\"fruit\" ', ' 0.012*\"caramel\" ', ' 0.008*\"sugar\" ', ' 0.005*\"spice\" ', ' 0.005*\"tan\" ', ' 0.005*\"yeast\"'] \n",
      "\n",
      "['0.027*\"bourbon\" ', ' 0.018*\"vanilla\" ', ' 0.015*\"barrel\" ', ' 0.015*\"dark\" ', ' 0.013*\"oak\" ', ' 0.012*\"caramel\" ', ' 0.011*\"age\" ', ' 0.010*\"brown\" ', ' 0.010*\"fruit\" ', ' 0.008*\"pepper\"'] \n",
      "\n",
      "['0.011*\"tea\" ', ' 0.011*\"spruce\" ', ' 0.010*\"dark\" ', ' 0.009*\"black\" ', ' 0.008*\"guinness\" ', ' 0.008*\"tequila\" ', ' 0.006*\"roast\" ', ' 0.006*\"agave\" ', ' 0.006*\"ale\" ', ' 0.006*\"interest\"'] \n",
      "\n",
      "['0.024*\"honey\" ', ' 0.014*\"lager\" ', ' 0.012*\"clear\" ', ' 0.010*\"caramel\" ', ' 0.009*\"bread\" ', ' 0.008*\"amber\" ', ' 0.008*\"bready\" ', ' 0.008*\"maple\" ', ' 0.007*\"malty\" ', ' 0.007*\"clean\"'] \n",
      "\n",
      "['0.011*\"lager\" ', ' 0.009*\"clear\" ', ' 0.008*\"grain\" ', ' 0.008*\"corn\" ', ' 0.007*\"pale\" ', ' 0.006*\"golden\" ', ' 0.006*\"yellow\" ', ' 0.005*\"decent\" ', ' 0.005*\"watery\" ', ' 0.005*\"water\"'] \n",
      "\n",
      "['0.021*\"citrus\" ', ' 0.015*\"ipa\" ', ' 0.014*\"orange\" ', ' 0.013*\"pine\" ', ' 0.013*\"bitter\" ', ' 0.012*\"grapefruit\" ', ' 0.008*\"fruit\" ', ' 0.008*\"hoppy\" ', ' 0.007*\"floral\" ', ' 0.006*\"tropical\"'] \n",
      "\n",
      "['0.020*\"wheat\" ', ' 0.020*\"yeast\" ', ' 0.016*\"belgian\" ', ' 0.015*\"banana\" ', ' 0.015*\"spice\" ', ' 0.011*\"orange\" ', ' 0.010*\"cleave\" ', ' 0.009*\"hazy\" ', ' 0.008*\"fruit\" ', ' 0.008*\"citrus\"'] \n",
      "\n",
      "['0.012*\"lemon\" ', ' 0.010*\"sour\" ', ' 0.009*\"tart\" ', ' 0.009*\"salt\" ', ' 0.009*\"refresh\" ', ' 0.009*\"lime\" ', ' 0.008*\"gose\" ', ' 0.008*\"wheat\" ', ' 0.008*\"yellow\" ', ' 0.006*\"crisp\"'] \n",
      "\n",
      "['0.028*\"caramel\" ', ' 0.020*\"brown\" ', ' 0.015*\"dark\" ', ' 0.012*\"amber\" ', ' 0.012*\"ale\" ', ' 0.011*\"malty\" ', ' 0.009*\"toast\" ', ' 0.008*\"toffee\" ', ' 0.008*\"red\" ', ' 0.007*\"fruit\"'] \n",
      "\n",
      "['0.032*\"vanilla\" ', ' 0.025*\"blueberry\" ', ' 0.023*\"oak\" ', ' 0.015*\"peanut\" ', ' 0.014*\"butter\" ', ' 0.013*\"whiskey\" ', ' 0.013*\"barrel\" ', ' 0.009*\"caramel\" ', ' 0.009*\"bourbon\" ', ' 0.009*\"blueberries\"'] \n",
      "\n",
      "['0.013*\"crisp\" ', ' 0.013*\"golden\" ', ' 0.010*\"lager\" ', ' 0.010*\"clean\" ', ' 0.010*\"clear\" ', ' 0.010*\"pale\" ', ' 0.009*\"yellow\" ', ' 0.008*\"grain\" ', ' 0.008*\"grassy\" ', ' 0.008*\"lemon\"'] \n",
      "\n",
      "['0.031*\"sour\" ', ' 0.022*\"tart\" ', ' 0.016*\"fruit\" ', ' 0.013*\"funk\" ', ' 0.012*\"cherry\" ', ' 0.012*\"oak\" ', ' 0.009*\"red\" ', ' 0.009*\"tartness\" ', ' 0.008*\"wine\" ', ' 0.008*\"cherries\"'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open('lda_model_beers_14topics.pkl', 'rb') as fp:\n",
    "    ldamodel = pickle.load(fp)\n",
    "\n",
    "for i,topic in enumerate(ldamodel.print_topics(num_topics=14, num_words=10)):\n",
    "   words = topic[1].split(\"+\")\n",
    "   print(words,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Function to remove stop words from sentences & lemmatize words.\n",
    "def clean(beer):\n",
    "    stop_free = \" \".join([i for i in beer.lower().split() if i not in stop])\n",
    "    normalized = \" \".join(lemma.lemmatize(word,'v') for word in stop_free.split())\n",
    "    x = normalized.split()\n",
    "    y = [s for s in x if len(s) > 2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_similar_documents(corpus, dirname):\n",
    "    clean_beers = [clean(beer) for beer in corpus]\n",
    "    test_term = [ldamodel.id2word.doc2bow(beer) for beer in clean_beers]\n",
    "    beer_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.20)\n",
    "    for k,topics in enumerate(beer_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            dir_name = dirname + \"/\" + str(topics[0][0])\n",
    "            file_name = dir_name + \"/\" + str(k) + \".txt\"\n",
    "            if not os.path.exists(dir_name):\n",
    "                os.makedirs(dir_name)\n",
    "            fp = open(file_name,\"w\")\n",
    "            fp.write(beers_test[k] + \"\\n\\n\" + str(topics[0][1]) )\n",
    "            fp.close()\n",
    "        else:\n",
    "            if not os.path.exists(dirname + \"/unknown\"):\n",
    "                os.makedirs(dirname + \"/unknown\")\n",
    "            file_name = dirname + \"/unknown/\" + str(k) + \".txt\"\n",
    "            fp = open(file_name,\"w\")\n",
    "            fp.write(beers_test[k])\n",
    "\n",
    "def get_related_documents(term, top, corpus):\n",
    "    clean_docs = [clean(doc) for doc in corpus]\n",
    "    related_docid = []\n",
    "    test_term = [ldamodel.id2word.doc2bow(doc) for doc in clean_docs]\n",
    "    doc_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.30)\n",
    "    term_topics =  ldamodel.get_term_topics(term, minimum_probability=0.01)\n",
    "    for k,topics in enumerate(doc_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            try:\n",
    "                if topics[0][0] == term_topics[0][0]:\n",
    "                    related_docid.append((k,topics[0][1]))\n",
    "            except IndexError:\n",
    "                return print(\"Too specific or no match! Try again.\")\n",
    "    beerids = []\n",
    "    related_docid.sort(key = itemgetter(1), reverse=True)\n",
    "    for j,doc_id in enumerate(related_docid):\n",
    "        print(doc_id[1],\"\\n\\n\",docs_test[doc_id[0]])\n",
    "        if j == (top-1):\n",
    "            break\n",
    "        beerids.append(doc_id[0])\n",
    "    if len(beerids) > 0:\n",
    "        return beerids #doc_id[0]\n",
    "    else:\n",
    "        return \"Too specific or no match! Try again\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_fp = open(\"docs_beers.pkl\", 'rb')\n",
    "docs_all = pickle.load(docs_fp)\n",
    "docs_test = docs_all#[3500:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_related_documents(\"stout\", 10 , docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Big Smoke'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbeers['name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "allbeers = pd.read_csv('../beers/beerlist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poop Your Pants Chocolate Bock\n",
      "Lobo Negro\n",
      "Ellie's Brown Ale\n",
      "Rugged Trail Nut Brown Ale\n",
      "Mad Tom's Robust Porter\n",
      "Boffo Brown Ale\n",
      "Jupiter (The Bringer Of Jollity)\n",
      "Samuel Adams Hazel Brown\n",
      "Get Up Offa That Brown\n"
     ]
    }
   ],
   "source": [
    "#beers = pd.read_csv('../beers/beerlist.csv')\n",
    "#beers['name'][a]\n",
    "for i in a:\n",
    "    print(allbeers['name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from operator import itemgetter\n",
    "import os\n",
    " \n",
    "# initialize WordNetLemmatizer and get the list of english stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    " \n",
    "# Load trained LDA model\n",
    "lda_fp = open(\"lda_model_beers_14topics_allbeers.pkl\", 'rb')\n",
    "ldamodel = pickle.load(lda_fp)\n",
    " \n",
    "# Load the beers corpus to choose 500 lines for test purpose\n",
    "beers_fp = open(\"docs_beers.pkl\", 'rb')\n",
    "#beers_all = pickle.load(docs_fp)\n",
    "#_test = docs_all[3500:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_all = pickle.load(beers_fp)\n",
    "beer_test = beers_all[3500:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99922216\n"
     ]
    }
   ],
   "source": [
    "# Get 'top' related documents given a word(term)\n",
    "a = get_related_documents(\"chocolate\", 10 , beer_test)\n",
    "# performs document clustering given a set of documents\n",
    "#cluster_similar_documents(docs_test,\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('lda_model_beers_14topics.pkl', 'rb') as fp:\n",
    "    lda_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wheat', 0.020052893),\n",
       " ('yeast', 0.019697063),\n",
       " ('belgian', 0.015831603),\n",
       " ('banana', 0.0151973935),\n",
       " ('spice', 0.014571979),\n",
       " ('orange', 0.010813807),\n",
       " ('cleave', 0.009907254),\n",
       " ('hazy', 0.008648241),\n",
       " ('fruit', 0.008244533),\n",
       " ('citrus', 0.007832508)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_dict.show_topic(topicid=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "allbeers = pd.read_csv('../beers/beerlist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "allbeers = pd.read_csv('../beers/beerlist.csv')\n",
    "beer_train = allbeers['words'][:3500]\n",
    "beer_test = allbeers['words'][:3500].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_test[0]\n",
    "words = [re.sub(r'[^\\w\\s]', '', word) for word in beer_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pre_process(beer_test[0])\n",
    "words = clean(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-fb63f5b80981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeer_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbeer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeer_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-fb63f5b80981>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeer_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbeer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeer_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-750dd147c797>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(beer)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Function to remove stop words from sentences & lemmatize words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstop_free\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_free\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "beer_clean = [pre_process(clean(beer_test)) for beer in beer_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4515\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-81b7e8478051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_article_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_article_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# select and article at random from test_df\n",
    "random_article_index = np.random.randint(len(words))\n",
    "print(random_article_index)\n",
    "\n",
    "new_bow = dictionary.doc2bow(words[random_article_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gensim.matutils' from '/home/steven/anaconda2/lib/python3.7/site-packages/gensim/matutils.py'>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim \n",
    "gensim.matutils.cossim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'['"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_test.iloc[random_article_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ad837f760c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwords1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean' is not defined"
     ]
    }
   ],
   "source": [
    "words0 = clean(pre_process(beer_test[0]))\n",
    "words1 = clean(pre_process(beer_test[1]))\n",
    "words2 = clean(pre_process(beer_test[2]))\n",
    "words3 = clean(pre_process(beer_test[3]))\n",
    "\n",
    "d0 = dictionary.doc2bow(words0)\n",
    "d1 = dictionary.doc2bow(words1)\n",
    "d2 = dictionary.doc2bow(words2)\n",
    "d3 = dictionary.doc2bow(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LdaMulticore' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0a8fa5ea7ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallbeers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'LdaMulticore' object is not callable"
     ]
    }
   ],
   "source": [
    "lda_dict(allbeers['words'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
