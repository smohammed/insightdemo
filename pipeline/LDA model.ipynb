{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of beers = 4813\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "import codecs\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "allbeers = pd.read_csv('../beers/beerlist.csv')\n",
    "print('number of beers = '+(str(len(allbeers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven/anaconda2/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/steven/anaconda2/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/steven/anaconda2/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/steven/anaconda2/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    return text\n",
    "\n",
    "allbeers['words'] = allbeers['words'].apply(lambda x:pre_process(x))\n",
    "for i in range(len(allbeers)):\n",
    "    allbeers['words'][i] = allbeers['words'][i].split()\n",
    "    try:\n",
    "        allbeers['words'][i] = random.sample(allbeers['words'][i], 2000)\n",
    "    except ValueError:\n",
    "        allbeers['words'][i] = random.sample(allbeers['words'][i], len(allbeers['words'][i]))\n",
    "    allbeers['words'][i] = ' '.join(allbeers['words'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Function to remove stop words from sentences & lemmatize words.\n",
    "def clean(beer):\n",
    "    stop_free = \" \".join([i for i in beer.lower().split() if i not in stop])\n",
    "    normalized = \" \".join(lemma.lemmatize(word,'v') for word in stop_free.split())\n",
    "    x = normalized.split()\n",
    "    y = [s for s in x if len(s) > 2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_words = allbeers['words']\n",
    "#beers = open(\"docs_beers.pkl\",'wb')\n",
    "#pickle.dump(beer_words, beers)\n",
    " \n",
    "# Use 3500 beers for training.\n",
    "#beer_train, beer_test = train_test_split(beer_words, test_size=0.20)\n",
    "beer_train = allbeers['words'][:3500]\n",
    "\n",
    "# Cleaning all the train beers\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "beer_clean = [clean(beer) for beer in beer_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# Creating term dictionary of corpus, where each unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(beer_clean)\n",
    "\n",
    "# Filter terms which occurs in less than 10 beers & more than 90% of the comments\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.9)\n",
    " \n",
    "# List of few words which are removed from dictionary as they are content neutral\n",
    "stoplist = set('bad alcohol strong abv better good nothing beer beers also use make people know many call include part find become like mean often different usually take wikt come give well get since type list say change see refer actually iii aisne kinds pas ask would way something need things want every str'.split())\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "beer_term_matrix = [dictionary.doc2bow(beer) for beer in beer_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-59034b09d27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Creating the object for LDA model using gensim library & Training LDA model on the document term matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mldamodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# dump LDA model using cPickle for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore as Lda\n",
    "# Creating the object for LDA model using gensim library & Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(beer_term_matrix, workers=3, num_topics=14, id2word = dictionary, passes=10, iterations=200)\n",
    "\n",
    "# dump LDA model using cPickle for future use\n",
    "ldafile = open('lda_model_beers_14topics.pkl','wb')\n",
    "pickle.dump(ldamodel,ldafile)\n",
    "ldafile.close()\n",
    " \n",
    "# Print all the 50 topics\n",
    "for i,topic in enumerate(ldamodel.print_topics(num_topics=14, num_words=10)):\n",
    "   words = topic[1].split(\"+\")\n",
    "   print(words,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_similar_documents(corpus, dirname):\n",
    "    clean_beers = [clean(beer) for beer in corpus]\n",
    "    test_term = [ldamodel.id2word.doc2bow(beer) for beer in clean_beers]\n",
    "    beer_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.20)\n",
    "    for k,topics in enumerate(beer_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            dir_name = dirname + \"/\" + str(topics[0][0])\n",
    "            file_name = dir_name + \"/\" + str(k) + \".txt\"\n",
    "            if not os.path.exists(dir_name):\n",
    "                os.makedirs(dir_name)\n",
    "            fp = open(file_name,\"w\")\n",
    "            fp.write(beers_test[k] + \"\\n\\n\" + str(topics[0][1]) )\n",
    "            fp.close()\n",
    "        else:\n",
    "            if not os.path.exists(dirname + \"/unknown\"):\n",
    "                os.makedirs(dirname + \"/unknown\")\n",
    "            file_name = dirname + \"/unknown/\" + str(k) + \".txt\"\n",
    "            fp = open(file_name,\"w\")\n",
    "            fp.write(beers_test[k])\n",
    "'''\n",
    "def get_related_documents(term, top, corpus):\n",
    "    clean_beers = [clean(beer) for beer in corpus]\n",
    "    related_beerid = []\n",
    "    test_term = [ldamodel.id2word.doc2bow(beer) for beer in clean_beers]\n",
    "    beer_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.30)\n",
    "    term_topics =  ldamodel.get_term_topics(term, minimum_probability=0.01)\n",
    "    for k,topics in enumerate(beer_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            if topics[0][0] == term_topics[0][0]:\n",
    "                related_beerid.append((k, topics[0][1]))\n",
    "    beerids = []\n",
    "    related_beerid.sort(key = itemgetter(1), reverse=True)\n",
    "    for j,beer_id in enumerate(related_beerid):\n",
    "        print(beer_id[1],\"\\n\\n\",beer_test[beer_id[0]][:10])\n",
    "        if j == (top-1):\n",
    "            break\n",
    "        beerids.append(beer_id[0])\n",
    "        return beerids #doc_id[0]\n",
    "\n",
    "\n",
    "def get_related_documents(term, top, corpus):\n",
    "    clean_docs = [clean(doc) for doc in corpus]\n",
    "    related_docid = []\n",
    "    test_term = [ldamodel.id2word.doc2bow(doc) for doc in clean_docs]\n",
    "    doc_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.30)\n",
    "    term_topics =  ldamodel.get_term_topics(term, minimum_probability=0.01)\n",
    "    for k,topics in enumerate(doc_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            if topics[0][0] == term_topics[0][0]:\n",
    "                related_docid.append((k,topics[0][1]))\n",
    " \n",
    "    related_docid.sort(key = itemgetter(1), reverse=True)\n",
    "    for j,doc_id in enumerate(related_docid):\n",
    "        print(doc_id[1],\"\\n\\n\",docs_test[doc_id[0]])\n",
    "        if j == (top-1):\n",
    "            break\n",
    "\n",
    "def get_related_documents(term, top, corpus):\n",
    "    clean_beers = [clean(beer) for beer in corpus]\n",
    "    related_beerid = []\n",
    "    test_term = [ldamodel.id2word.doc2bow(beer) for beer in clean_beers]\n",
    "    beer_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.30)\n",
    "    term_topics =  ldamodel.get_term_topics(term, minimum_probability=0.000001)\n",
    "    for k,topics in enumerate(beer_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            if topics[0][0] == term_topics[0][0]:\n",
    "                related_beerid.append((k,topics[0][1]))\n",
    "    beerids = []\n",
    "    related_beerid.sort(key = itemgetter(1), reverse=True)\n",
    "    for j, beer_id in enumerate(related_beerid):\n",
    "        print(beer_id[1])#,\"\\n\\n\",corpus[beer_id[0]])\n",
    "        if j == (top-1):\n",
    "              break\n",
    "        beerids.append(beer_id[0])\n",
    "        return beerids\n",
    "'''\n",
    "\n",
    "def get_related_documents(term, top, corpus):\n",
    "    clean_docs = [clean(doc) for doc in corpus]\n",
    "    related_docid = []\n",
    "    test_term = [ldamodel.id2word.doc2bow(doc) for doc in clean_docs]\n",
    "    doc_topics = ldamodel.get_document_topics(test_term, minimum_probability=0.30)\n",
    "    term_topics =  ldamodel.get_term_topics(term, minimum_probability=0.01)\n",
    "    for k,topics in enumerate(doc_topics):\n",
    "        if topics:\n",
    "            topics.sort(key = itemgetter(1), reverse=True)\n",
    "            if topics[0][0] == term_topics[0][0]:\n",
    "                related_docid.append((k,topics[0][1]))\n",
    "    beerids = []\n",
    "    related_docid.sort(key = itemgetter(1), reverse=True)\n",
    "    for j,doc_id in enumerate(related_docid):\n",
    "        print(doc_id[1],\"\\n\\n\",docs_test[doc_id[0]])\n",
    "        if j == (top-1):\n",
    "            break\n",
    "        beerids.append(doc_id[0])\n",
    "    return beerids#doc_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_fp = open(\"docs_beers.pkl\", 'rb')\n",
    "docs_all = pickle.load(docs_fp)\n",
    "docs_test = docs_all#[3500:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_related_documents(\"stout\", 10 , docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152, 33, 1230, 1211, 577, 827, 332, 326, 887]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoppin' Bubbly\n",
      "Powder Hound Winter Ale\n",
      "HUB Lager\n",
      "Milwaukee's Best Light\n",
      "Invasion Pale Ale\n",
      "Titanic Stout\n",
      "Ayinger Jahrhundert Bier\n",
      "Immort Ale\n",
      "Viking Fraoch\n"
     ]
    }
   ],
   "source": [
    "#beers = pd.read_csv('../beers/beerlist.csv')\n",
    "#beers['name'][a]\n",
    "for i in a:\n",
    "    print(beers['name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from operator import itemgetter\n",
    "import os\n",
    " \n",
    "# initialize WordNetLemmatizer and get the list of english stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    " \n",
    "# Load trained LDA model\n",
    "lda_fp = open(\"lda_model_beers_14topics.pkl\", 'rb')\n",
    "ldamodel = pickle.load(lda_fp)\n",
    " \n",
    "# Load the beers corpus to choose 500 lines for test purpose\n",
    "beers_fp = open(\"docs_beers.pkl\", 'rb')\n",
    "#beers_all = pickle.load(docs_fp)\n",
    "#_test = docs_all[3500:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers = \n",
    "beers_all = pickle.load(beers_fp)\n",
    "beer_test = beers_all[3500:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99922216\n"
     ]
    }
   ],
   "source": [
    "# Get 'top' related documents given a word(term)\n",
    "a = get_related_documents(\"chocolate\", 10 , beer_test)\n",
    "# performs document clustering given a set of documents\n",
    "#cluster_similar_documents(docs_test,\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
